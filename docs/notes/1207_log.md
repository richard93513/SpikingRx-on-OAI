# 2025-12-06 SpikingRx-on-OAI 工作紀錄（今日進度）

---

## 1. OAI LDPC decoder：從獨立專案改成 CMake 內建目標

### 1.1 舊作法回顧（獨立 `spx_ldpc_test` 專案）

- 之前在 `~/openairinterface5g/spx_ldpc_test/` 切了一個獨立資料夾，只想編譯 LDPC decoder：
  - 手動複製 `nrLDPC_decoder.c`、`nrLDPC_*` header / LUT。
  - 再補一堆 OAI 的共用檔（`time_meas.h`、`common/utils` 等）。
- 實際操作碰到很多問題：
  - `nrLDPC_decoder.c` 還會 include 其他 OAI internal header（`PHY/defs.h`、`nrLDPC_types.h`…），include path 很難一次補齊。
  - 需要 OAI 的 log / 斷言 / SIMD 巨集（`AssertFatal`、`LOG_I`、`c16_t`…），等於一直在追新的相依檔。
  - `Makefile` 要自己維護 compile flags（`-msse4.1 -mavx2` 等）和 link 選項，後續 OAI 版本更新也容易爆掉。

→ 結論：獨立專案方式太脆弱，不適合長期維護。

### 1.2 新作法：直接在 OAI CMake 裡增加 `ldpctest_spx` 目標

- 改成在 OAI 的 `CMakeLists.txt` 裡新增一個 executable：
  - 名稱：`ldpctest_spx`
  - source：直接用原本的 `openair1/PHY/CODING/nrLDPC_decoder.c` 及相關檔案。
  - link：沿用 OAI 既有的 LDPC decoder compile flags / libraries。
- 編譯後執行檔路徑：
  - `~/openairinterface5g/cmake_targets/ran_build/build/ldpctest_spx`
- 呼叫介面（目前版本）：
  - `./ldpctest_spx infer_llr_int8.bin ldpc_cfg.txt decoded_bits.bin`
- 注意事項：
  - RNTI 本身不直接影響 decoder，只是在 gNB / UE dump TX bits、fullgrid、OAI LLR 時用來區分是哪一筆傳輸（例如 SI-RNTI=65535）。
  - **UTIL 要記得去 `openair1/PHY/CODING` 裡去調**：  
    需要的 util / include / flags 一律在這個目錄底下跟著 OAI 的 CMake 設定，避免再手動複製。

---

## 2. 資料流更新：從 TX bits → OAI LLR → SpikingRx → LDPCTest

### 2.1 UE 端新增 OAI demapper LLR dump

- 在 UE demodulation / demapping (`nr_dlsch_demodulation.c` 一帶) 新增 dump：
  - 把 UE 端要給 LDPC decoder 的 LLR（float）寫成 `oai_llr.bin`。
- 現在每個 bundle `spx_records/bundle/fXXXX_sYY/` 內容包含：
  - `fullgrid.bin`：RX full OFDM grid（FFT 後 downsample）。
  - `txbits.bin`：gNB TX TB bits（packed byte）。
  - `ldpc_cfg.txt`：OAI 解碼時使用的 LDPC 參數（A, G, C, F, Qm…）。
  - `meta.json`：frame / slot / fg_idx / tx_idx / rnti。
  - `oai_llr.bin`：OAI demapper LLR（float32，長度 G）。

### 2.2 SpikingRx 輸出維度調整為 14400

- 觀察到 UE dump 的 `G = 14400`，一開始 SpikingRx readout 的輸出長度不等於 14400。
- 為了能直接丟進 LDPC decoder：
  - 把最後一層 ANN readout 改成固定輸出 `out_bits = 14400`。
  - 在 dataset 裡讀 `ldpc_cfg.txt`，只處理 `G = 14400` 的 bundle（其他先 skip）。
- 現在一筆樣本的完整路徑：
  1. `fullgrid.bin` → `load_oai_fullgrid` → `x`（[B=1, T=3, C=2, H=32, W=32]）。
  2. SpikingRx spiking backbone → feature maps。
  3. 時間平均 → ANN readout → `llr_vec`（[1, 14400]）。
  4. 正規化、clip、轉 int8 → `infer_llr_int8.bin`。
  5. `infer_llr_int8.bin + ldpc_cfg.txt` → `ldpctest_spx` → `decoded_bits.bin`。
  6. 用 `txbits.bin` 還原 A 個 bit，與 `decoded_bits` 比較，計算 BER。

---

## 3. 模型與訓練：改用 OAI LLR 當 label

### 3.1 Dataset 更新

新版 `OAI_Bundle_Dataset` 對每個 bundle 回傳：

- `x`：由 `fullgrid.bin` 產生的輸入 tensor。
- `y_llr`：從 `oai_llr.bin` 讀出的 float32 LLR，長度 G=14400。
- `cfg`：由 `ldpc_cfg.txt` parse 出來的 dict（A / G / C / F / Qm…）。
- `bundle_dir`：該 bundle 的路徑。

Loss 改成：

- `loss = MSE( SpikingRx_pred_LLR , OAI_demapper_LLR )`  
  → SpikingRx 直接學 OAI demapper 的 LLR 輸出。

### 3.2 時間維度 T 從 5 改成 3

- `load_oai_fullgrid` 的 `T` 由 5 改成 3：
  - 節省顯存與運算量，才跑得動整個 dataset。
- `SpikingRxModel` 新增成員 `self.T`，forward 開頭：
  - `assert T == self.T`，保證 dataset / model 對齊。
- 現在：
  - 訓練與 inference 統一使用 `T=3`。

### 3.3 通道數 base_ch = 16（目前版本）

- 有討論過降到 12ch 來再縮模型，但目前正式訓練與 checkpoint 都用：
  - `base_ch = 16`。
- 通道配置：
  - Stem: 2 → 16
  - stages: `[16→16, 16→32, 32→32, 32→64, 64→64, 64→64]`
- 所有新版 script（訓練 / inference / BER）都以 `base_ch=16` 重新建立 model，載入 `best_spikingrx_model.pth`。

---

## 4. GPU / CPU 混合運算：在 GTX 950M 上跑 SpikingRx

### 4.1 驅動與 CUDA 狀態

- 顯卡：NVIDIA GeForce GTX 950M，2GB。
- 驅動安裝完成後：
  - `nvidia-smi` 顯示：
    - Driver Version: 470.256.02
    - CUDA Version: 11.4
    - GPU 0: 2004 MiB。
  - `torch.cuda.is_available()` → True  
    `torch.cuda.get_device_name(0)` → `"NVIDIA GeForce GTX 950M"`。

### 4.2 為何要 hybrid：只讓 spiking 部分上 GPU

- 如果整個模型（spiking backbone + ANN readout）全放 GPU：
  - out_bits=14400 的 FC 層太大，加上 feature maps，很容易直接 OOM。
- 現在的折衷設計：
  - `SpikingRxModel` 初始化時傳入：
    - `device_conv`：`cuda` → spiking conv + SEW blocks。
    - `device_fc`：`cpu` → ANN readout。
  - forward 流程：
    1. `x` 在 `device_conv` 上跑 `StemConv + 6×SEWBlock`。
    2. time-mean 後的 rate map 搬到 CPU，變成 `rate_fc`。
    3. 在 CPU 上跑 `ReadoutANN` 得到 `[B, 14400]` LLR。
  - 訓練 loop：
    - `x`、`y_llr` 先在 CPU。
    - backward 主要對 ANN 部分算梯度，spiking backbone 暫時當特徵 extractor 使用（未啟用 AMP / half precision，避免 dtype 衝突）。
- 這樣做的結果：
  - 不會 OOM，訓練可以穩定跑完整個 epoch。
  - 一次只犧牲 readout 在 CPU 上比較慢一點。

### 4.3 實際訓練速度與 loss 收斂情況

- 設定：
  - `max_epochs = 30`
  - `batch_size = 1`
  - dataset bundles ≈ 1583。
- 實測：
  - 每 epoch 約 1 小時 ~ 1 小時 15 分。
  - 每個 iteration 約 2.7–2.9 秒。
- Total Loss（所有 sample 的 MSE 累積）大致走勢：
  - Epoch 1: ≈ 205,860
  - Epoch 2: ≈ 205,373
  - Epoch 3: ≈ 205,296
  - Epoch 4: ≈ 205,214
  - Epoch 5: ≈ 205,029
  - Epoch 6: ≈ 204,570
  - Epoch 7: ≈ 203,606
  - Epoch 8: ≈ 202,010
  - Epoch 9: ≈ 200,084
  - Epoch 10 之後仍持續緩慢下降
- 每一個 epoch 結束目前都會：
  - `✓ Improved — saved best model (...)`  
  → `best_spikingrx_model.pth` 會被更新成最新最低 loss 的權重。

---

## 5. 新版 inference + LDPC + BER：整合成單一 script

### 5.1 `batch_inference_on_bundle.py` 的完整流程

- 這支 script 現在負責「一條龍」：

1. **載入 model + checkpoint**
   - 建立 `SpikingRxModel`（`in_ch=2, base_ch=16, T=3, out_bits=14400`）。
   - `device_conv = cuda`、`device_fc = cpu`。
   - 從 `src/train/best_spikingrx_model.pth` 載入權重，`model.eval()`。

2. **掃描 bundle 目錄**
   - `spx_records/bundle/f*_s*/` 一一處理。

3. **對每個 bundle：**
   - 讀 `ldpc_cfg.txt` → parse 出 `A`、`G` 等。
   - 若 `G != 14400`，直接 `[SKIP]`。
   - 讀 `fullgrid.bin`，用 `load_oai_fullgrid(..., T=3)` 轉成 SpikingRx tensor `x`。

4. **SpikingRx 推論**
   - `with torch.no_grad(): llr_vec, aux = model(x)`  
     → `llr_vec` shape `[1, 14400]`。

5. **LLR 轉 int8 給 OAI decoder**
   - 呼叫 `save_llr_for_oai_decoder`：
     - 必要時 sign flip（現在用 `flip_sign=True`）。
     - `/ llr_clip` 後 clip 到 [-1, 1] 再乘 127 → int8。
     - 存成 `infer_llr_int8.bin`。

6. **呼叫 OAI `ldpctest_spx` 解碼**
   - 建立指令：
     - `ldpctest_spx infer_llr_int8.bin ldpc_cfg.txt decoded_bits.bin`
   - 用 `subprocess.run` 執行。
   - 解碼結果寫到 `decoded_bits.bin`。

7. **計算 BER**
   - 用 `txbits.bin` 解 pack 成 A 個 bit（`np.unpackbits(...)[:A]`）。
   - 從 `decoded_bits.bin` 讀出前 A 個 bit 當 RX。
   - `BER = (#bit 不相等) / A`。
   - 印出每個 bundle 的 BER。

8. **最後總結**
   - 全部 bundle 跑完後，列出：
     - `fXXXX_sYY → BER = ...`。

---

## 6. 今天的總結

今天的關鍵成果可以整理成：

1. **LDPC decoder 使用方式從「獨立專案」改為「OAI CMake 內建目標」**  
   - 不再自己複製一堆 `nrLDPC_*` 檔案與 util。  
   - 直接在 OAI 的 `CMakeLists.txt` 裡新增 `ldpctest_spx`，讓 OAI 自己處理所有 include / flags。  
   - 特別提醒：後續如果要調整 util 或 LDPC 相關 code，要回到 `openair1/PHY/CODING/` 裡改。

2. **讓 SpikingRx 與 OAI pipeline 完全對齊**  
   - UE dump `oai_llr.bin` 作為 ground truth（label）。  
   - SpikingRx readout 輸出長度固定為 14400，完全對應 OAI 的 `G`。  
   - 訓練時用 `MSE(SpikingRx_LLR, OAI_LLR)`，等於直接學 OAI demapper。

3. **在 GTX 950M 上找到可接受的訓練設定**  
   - 使用 hybrid device：spiking backbone 跑在 GPU，ANN readout 跑在 CPU。  
   - 輸入時間維度從 T=5 壓到 T=3，避免 OOM。  
   - 在這樣的設定下，完整 dataset 每個 epoch 約 1 小時上下，Total Loss 穩定往下掉。

4. **把 inference、LDPC 解碼與 BER 計算整合到一支 script**  
   - 只要跑 `batch_inference_on_bundle.py`，就能完成：  
     `fullgrid → SpikingRx → infer_llr_int8.bin → ldpctest_spx → decoded_bits.bin → BER`。  
   - 這條流程已經是完整的「OAI gNB/UE + SpikingRx + OAI LDPC decoder」實驗架構，後續只要收不同 SNR 的 bundle，就能開始畫 BER curve、真正對比 baseline。

整體來說，今天最大的意義是：  
**從「零散的 patch」正式升級成一條可以直接跑 BER 的端到端 pipeline，並且已經開始用 OAI 的真實 LLR 來訓練 SpikingRx。**
